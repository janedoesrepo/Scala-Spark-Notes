{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala Spark\n",
    "Notes on the course [Big Data Analysis with Scala and Spark](https://www.coursera.org/learn/scala-spark-big-data) by Heather Mills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download java: https://java.com/en/download/.\n",
    "- Download Spark: https://spark.apache.org/downloads.html\n",
    "- Download Hadoop: https://github.com/cdarlint/winutils\n",
    "\n",
    "\n",
    "- Set evironment variables\n",
    " - SPARK_HOME = \\<path-to-spark-folder>, e.g. SPARK_HOME = \"C:\\spark-3.1.1-bin-hadoop2.7\"\n",
    " - HADOOP_HOME = \\<path-to-hadoop-folder>, e.g. HADOOP HOME = \"C:\\hadoop\\hadoop-2.7.7\"\n",
    " \n",
    " \n",
    "- Add to PATH\n",
    " - %SPARK_HOME%\\bin\n",
    " - %HADOOP_HOME%\\bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala and Spark for Jupyter Notebook\n",
    "https://github.com/mariusvniekerk/spylon-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Install spylon-kernel**\n",
    "\n",
    "```\n",
    "pip install spylon-kernel\n",
    "# or\n",
    "conda install -c conda-forge spylon-kernel\n",
    "```\n",
    "\n",
    "- **Create a Scala Kernel**\n",
    "\n",
    "```\n",
    "python -m spylon_kernel install\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify installation\n",
    "\n",
    "Open a jupyter notebook, select the spylon-kernel and execute some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x: Int = 2\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see something like the following output, everything works fine:\n",
    "\n",
    "```Output\n",
    "Intitializing Scala interpreter ...\n",
    "\n",
    "Spark Web UI available at http://<your-computer>.com:4040\n",
    "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1618231637628)\n",
    "SparkSession available as 'spark'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "\n",
    "RDDs seem a lot like ***immutable*** sequential or parallel Scala collections. Most operations on RDDs, like Scala's immutable `List`, and Scala's parallel collections, are higher-order functions like map, flatMap, filter, reduce, fold, and aggregate.\n",
    "\n",
    "```Scala\n",
    "map[B](f: A => B): List[B] // Scala List\n",
    "map[B](f: A => B): RDD[B]  // Spark RDD\n",
    "```\n",
    "\n",
    "```Scala\n",
    "aggregate[B](z: => B)(seqop: (B, A) => B, combop: (B, B) => B): B // Scala\n",
    "aggregate[B](z: B)(seqop: (B, A) => B, combop: (B, B) => B): B    // Spark RDD\n",
    "```\n",
    "\n",
    "Using RDDs in Spark feels a lot like normal Scala sequential/parallel collections, with added knowledge that your data is distributed across several machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Count: The \"Hello, World!\" of programming with large-scala data\n",
    "\n",
    "```Scala\n",
    "// Create an RDD\n",
    "val rdd = spark.textFile(\"hdfs://...\")\n",
    "\n",
    "val count = rdd.flatMap(line => line.split(\" \")) // separate lines into words\n",
    "               .map(word => (word, 1))           // include something to count\n",
    "               .reduceByKey(_ + _)               // sum up the 1s in the pairs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDDs\n",
    "\n",
    "RDDs can be created in two ways:\n",
    " - Transforming an existing RDD.\n",
    " - From a `SparkContext`(or `SparkSession`) object with `parallelize` or `textFile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Actions\n",
    "\n",
    "1. **Transformers** return new RDDs as results (they are `lazy`, the result is not immediately computed).\n",
    "2. **Actions** compute a result based on an RDD, and either returns or saves the result (they are `eager`, the result is immediately computed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Transformers\n",
    "\n",
    " - `map(f)`\n",
    "```Scala\n",
    "// Apply f to each element and return an RDD of the result\n",
    "map[B](f: A => B): RDD[B] \n",
    "```\n",
    "\n",
    " - `flatMap(f)`\n",
    "```Scala\n",
    "// Apply f to each element and return an RDD of the contents of the iterators returned\n",
    "flatMap[B](f: A => TraversableOnce[B]): RDD[B] \n",
    "```\n",
    "\n",
    " - `filter(pred)`\n",
    "```Scala\n",
    "// Apply predicate to each element and return an RDD of elements that have passed it\n",
    "filter[B](pred: A => Boolen): RDD[A]\n",
    "```\n",
    "\n",
    " - `distinct()`\n",
    "```Scala\n",
    "// Return RDD with duplicates removed\n",
    "distinct(): RDD[B]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Actions\n",
    "\n",
    " - `collect()`\n",
    "```Scala\n",
    "// Return all elements from RDD\n",
    "collect(): Array[T] \n",
    "```\n",
    "\n",
    " - `count()`\n",
    "```Scala\n",
    "// Return the number of elements in the RDD\n",
    "count(): Long \n",
    "```\n",
    "\n",
    " - `take(num)`\n",
    "```Scala\n",
    "// Return the first num elements of the RDD\n",
    "take(num: Int): Array[T]\n",
    "```\n",
    "\n",
    " - `reduce(op)`\n",
    "```Scala\n",
    "// Combine the elements in the RDD together using `op function` and return results\n",
    "reduce(op: (A, A) => A): A\n",
    "```\n",
    "\n",
    " - `foreach(f)`\n",
    "```Scala\n",
    "// Apply function to each element in RDD.\n",
    "foreach(f: T => Unit): Unit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-like Transformations\n",
    "\n",
    " - `union(other)`\n",
    "```Scala\n",
    "// Return an RDD containing elements from both RDDs\n",
    "union(other: RDD[T]): RDD[T] \n",
    "```\n",
    "\n",
    " - `intersection(other)`\n",
    "```Scala\n",
    "// Return an RDD containing elements only found in both RDDs\n",
    "intersection(other: RDD[T]): RDD[T] \n",
    "```\n",
    "\n",
    " - `subtract(other)`\n",
    "```Scala\n",
    "// Return an RDD with the contents of the other RDD removed\n",
    "subtract(other: RDD[T]): RDD[T] \n",
    "```\n",
    "\n",
    " - `cartesian(other)`\n",
    "```Scala\n",
    "// Cartesian product with the other RDD\n",
    "cartesian[U](other: RDD[U]): RDD[(T, U)] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache and Persistance: Logistic Regression Example\n",
    "\n",
    "```Scala\n",
    "val points = sc.textFile(...).map(parsePoints)\n",
    "val w = Vector.zeros(d)\n",
    "for (i <- 1 to numIterations) {\n",
    "    val gradient = points.map { p =>\n",
    "        (1 / (1 + exp(-p.y * w.dot(p.x))) - 1) * p.y * p.y\n",
    "    }.reduce(_+_)\n",
    "    w -= alpha * gradient\n",
    "}\n",
    "```\n",
    "\n",
    "This code works fine. After reading data from disc and parsing the contents, we iterate `numIterations` over the logistic regression computation. However, transformations like `map` get re-evaluated every time an action is used. This means that `points` is being re-evaluated upon every iteration.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "To tell Spark to cache an RDD in memory, simply call `persist()` or `cache` on it.\n",
    "\n",
    "```Scala\n",
    "val points = sc.textFile(...).map(parsePoints).persist()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Operations on RDDs\n",
    "\n",
    " - `fold(zeroval)(combop)`\n",
    "```Scala\n",
    "fold(z: A)(f: (A, A) => A): A \n",
    "```\n",
    " - `aggregate(zeroval)(seqop, combop)`\n",
    "```Scala\n",
    "aggregate(z: => B)(seqop: (B, A) => B, combop: (B, B) => B): B \n",
    "```\n",
    "\n",
    "Spark doesn't even give you the option to use foldLeft/foldRight, which means that if you have to change the return type of your reduction operation, your only choice is to use aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "- allows seamless intermixing of SQL queries with Scala\n",
    "- adapts optimizations used in databases to Spark jobs\n",
    "\n",
    "**Goals:**\n",
    "- Support relational processing both within Spark programs (on RDDs) and on external data sources with a friendly API\n",
    "- High performance, achieved by using techniques from research in databases\n",
    "- Easily support new data sources such as semi-structured data and external databases\n",
    "\n",
    "**Main APIs:**\n",
    "- SQL literal syntax\n",
    "- DataFrames\n",
    "- Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames\n",
    "- DataFrames are, conceptually, RDDs full of records with a known schema.\n",
    "- DataFrames are untyped (unlike RDD[T]).\n",
    "- Transformations on DataFrames are untyped transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"My App\")\n",
    "    //.config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames\n",
    "1. From an existing RDD\n",
    "    1. with schema inference\n",
    "    2. with an explicit schema\n",
    "2. Reading in a specific data source from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.A)** Create <code>DataFrame</code> from <code>RDD</code>, schema reflectively inferred\n",
    "Given a pair RDD, RDD[(T1, T2, ..., TN)], a DataFrame can be created with its schema automatically inferred by simply using the toDF method.\n",
    "\n",
    "```scala\n",
    "val tupleRDD = ... // Assume RDD[(Int, String, String, String)]\n",
    "val tupleDF = tupleRDD.toDF(\"id\", \"name\", \"city\", \"country\") // column names\n",
    "```\n",
    "\n",
    "If you already have an RDD containing some kind of case class isntance, then Spark can infer the attributes from the case class's fields.\n",
    "\n",
    "```scala\n",
    "case class Person(id: Int, name: String, city: String)\n",
    "val peopleRDD = ... // Assume RDD[Person]\n",
    "val peopleDF = peopleRDD.toDF\n",
    "```\n",
    "\n",
    "**(1.B)** Create <code>DataFrame</code> from existing <code>RDD</code>, schema explicitly specified\n",
    "Sometimes it's not possible to create a DataFrame with a pre-determined case class as its schema. For these cases, it's possible to explicitly specify a schema.\n",
    "It takes three steps:\n",
    "1. Create an RDD of <code>Rows</code> from the original RDD.\n",
    "2. Create the schema represented by a <code>StructType</code> matching the structure of <code>Rows</code> in the RDD created in step 1.\n",
    "3. Apply the schema to the RDD of <code>Rows</code> via <code>createDataFrame</code> method prrovided by <code>SparkSession</code>\n",
    "\n",
    "```scala\n",
    "// Given\n",
    "case class Person(name: String, age: Int)\n",
    "val peopleRDD = sc.textFile(...) // Assume RDD[Person]\n",
    "\n",
    "// The schema is encoded in a string\n",
    "val schemaString = \"name age\" \n",
    "\n",
    "// Generate the schema based on the schemaString\n",
    "val fields = schemaString.split(\" \")\n",
    "  .map(fieldName => StructField(fieldName, StringType, nullable = true))\n",
    "val schema = StructType(fields)\n",
    "       \n",
    "// Convert records of the RDD (people) to Rows\n",
    "val rowRDD = peopleRDD\n",
    "  .map(_.split(\",\"))\n",
    "  .map(attributes => Row(attributes(0), attributes(1).trim))\n",
    "\n",
    "// Apply the schema to the RDD\n",
    "val peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "                                         \n",
    "```\n",
    "\n",
    "**(2)** Create <code>DataFrame</code> by reading in a data source from file\n",
    "Using the <code>SparkSession</code> object, you can read in semi-structured/structured data by using the <code>read</code> method. For example, to read in data and infer a schema from a JSON file:\n",
    "\n",
    "```scala\n",
    "// 'spark' is the SparkSession object we created earlier\n",
    "val df = spark.read.json(\"<some path>.json\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SQL Literals\n",
    "\n",
    "Once you have a <code>DataFrame</code> to operate on, you can now freely write familiar SQL syntax to operate on your dataset!\n",
    "Given a <code>DataFrame</code> called <code>peopleDF</code>, we just have to register our <code>DataFrame</code> as a temporary SQL view first:\n",
    "\n",
    "```scala\n",
    "// Register the DataFrame as a SQL temporary view\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "// This essentially gives a name to our DataFrame in SQL\n",
    "// so we can refer to it in a SQL FROM statement\n",
    "\n",
    "// SQL literals can be passed to Spark SQL's sql method\n",
    "val adultsDF = spark.sql(\"SELECT * FROM people WHERE age > 17\")\n",
    "```\n",
    "\n",
    "The SQL statements avaiable are largely what's available in HiveQL. This includes standard SQL statements.\n",
    "\n",
    "**Supported Spark SQL syntax:**<br/>\n",
    "https://docs.datastax.com/en/archived/datastax_enterprise/4.6/datastax_enterprise/spark/sparkSqlSupportedSyntax.html\n",
    "\n",
    "**For a HiveQL cheatsheet:**<br/>\n",
    "http://hortonworks.com/wp-content/uploads/2016/05/Hortonworks.CheatSheet.SQLtoHive.pdf\n",
    "\n",
    "**For an updated list of suported Hive features in Spark SQL, the official Spark SQL docs enumerate:**<br/>\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html#supported-hive-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "```scala\n",
    "case class Employee(id: Int, fname: String, lname: String, age: Int, city: String)\n",
    "\n",
    "// DataFrame with schema defined in Employee case class\n",
    "val employeeDF = sc.parallelize(...).toDF\n",
    "\n",
    "// Register temp view\n",
    "employeeDF.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "val sydneyEmployeesDF = spark\n",
    "  .sql(\"\"\"SELECT id, lname\n",
    "          FROM employees\n",
    "          WHERE city = \"Sydney\"\n",
    "          ORDER BY id\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames Deep Dive\n",
    "\n",
    " - available DataFrame data types\n",
    " - basic operations on DataFrames\n",
    " - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "// Accessing sql datatypes requires import\n",
    "import org.apache.spark.sql.types._\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting and working with columns\n",
    "\n",
    "You can select and work with columns in three ways:\n",
    "\n",
    "1. Using $-notation\n",
    "```scala\n",
    "df.filter($\"age\" > 17)\n",
    "```\n",
    "\n",
    "2. Referring to the DataFrame\n",
    "```scala\n",
    "df.filter(df(\"age\") > 17)\n",
    "```\n",
    "\n",
    "3. Using SQL query string\n",
    "```scala\n",
    "df.filter(\"age > 17\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "```scala\n",
    "case class Employee(id: Int, fname: String, lname: String, age: Int, city: String)\n",
    "val employeeDF = sc.parallelize(...).toDF\n",
    "\n",
    "val sydneyEmployeesDF = employeeDF\n",
    "  .select(\"id\", \"lname\")\n",
    "  .where(\"city == 'Sydney'\")\n",
    "  .orderBy(\"id\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "// Accessing aggregation functions on groupBy\n",
    "import org.apache.spark.sql.functions._\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning DataFrames\n",
    "\n",
    "**Dropping records with unwanted values:**\n",
    "- ```drop``` drops rows that contail null or NaN values in **any** column\n",
    "- ```drop(\"all\")``` drops rows that contain null or NaN values in **all** columns\n",
    "- ```drop(Array(\"id\", \"name\"))``` drops rows that contail null or NaN values in the **specified** columns\n",
    "\n",
    "**Replacing unwanted values:**\n",
    "- ```fill(0)``` replaces all occurrences of null or NaN in **numeric columns** with a **specified value**\n",
    "- ```fill(Map(\"minBalance\" -> 0))``` replaces all occurrences of null or NaN in **specified columns** with the **specified values**\n",
    "- ```replace(Array(\"id\"), Map(1234 -> 8923))``` replaces a **specified value** (1234) in a **specified column** (id) with a **specified replalcement value** (8923)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins on DataFrames\n",
    "\n",
    "**Performing joins:**\n",
    "Given two DataFrames, df1 and df2 each with a column called id, we can perform an inner join as follows:\n",
    "```scala\n",
    "df1.join(df2, $\"df1.id\" === $\"df2.id\")\n",
    "```\n",
    "\n",
    "It's possible to change the join type by passing an additional string parameter to join specifying wwhich type of join to perform. E.g.,\n",
    "```scala\n",
    "df1.join(df2, $\"df1.id\" === $\"df2.id\", \"right_outer\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizations on DataFrames\n",
    "\n",
    "When using DataFrames, Spark takes care of optimization for us. This is mainly done by two components:\n",
    " - **Catalyst** query optimizer\n",
    " - **Tungsten** off-heap serializer\n",
    " \n",
    "Compared to RDDs, DataFrames are (like Databases) very structured and yields lots of opportunities for optimization.\n",
    "\n",
    "Assuming **Catalyst** has:\n",
    " - has full knowledge and understanding of all data types\n",
    " - knows the exact schema of our data\n",
    " - has detailed knowledge of the computations wew'd like to do\n",
    " \n",
    "This makes it possible for us to do optimizations like:\n",
    " - Reordering operations\n",
    " - Reduce the amount of data we must read\n",
    " - Pruning unneeded partitioning\n",
    " \n",
    "Since our data types are restricted to Spark SQL data types, **Tungsten** can provide:\n",
    " - highly-specialized data encoders\n",
    " - column-based format\n",
    " - off-heap (free from garbage collection)\n",
    " \n",
    "Taken together, Catalyst and Tungsten offer ways to significantly speed up your code, even if you write it inefficiently initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "**DataFrames are untyped**. Your code compiles, but you get runtime exceptions when you attempt to run a query on a column that doesn't exist. It would be nice if this wwas caught at compile time like we're used to in Scala.\n",
    "\n",
    "**Limited Data Types**. If your data can't be expressed by <code>case classes</code> and standard Spark SQL data types, it may be difficult to ensure that a Tungsten encoder exists for your data type.\n",
    "\n",
    "**Requires Semi-Structured/Structured Data**. If your unstructured data cannot be reformulated to adhere to some kind of schema, it would be better to use RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "val averagePrices = averagePricesDF.collect()\n",
    "// averagePrices: Array[org.apache.spark.sq.Row]\n",
    "\n",
    "averagePrices.head.schema.printTreeString()\n",
    "// root\n",
    "//  |-- zip: integer (nullable = true)\n",
    "//  |-- avg(price): double (nullable = true)\n",
    "\n",
    "val averagePricesAgain = averagesPrices.map {\n",
    "    row => (row(0).asInstanceOf[Int], ro(1).asInstanceOf[Double])\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames are acually Datasets:\n",
    "\n",
    "```scala\n",
    "type DataFrame = DataSet[Row]\n",
    "```\n",
    "\n",
    "**What is a Dataset?**\n",
    "- Datasets can be thought of as **typed** distributed colelctions of data.\n",
    "- The Dataset API unifies the DataFrame and RDD APIs (mix and match).\n",
    "- Datasets require structured/semi-structured data. Schemas and Encoders are a core part of Datasets.\n",
    "\n",
    "Think of Datasets as a compromise between RDDs & DataFrames. You get more type information on Datasets than on DataFrames, and you get more optimizations on Datasets than you get on RDDs.\n",
    "\n",
    "```scala\n",
    "listingsDS.groupByKey(l => l.zip)      // looks like groupByKey on RDDs!\n",
    "        .agg(avg($\"price\".as[Double])  // looks like our DataFrame operators!\n",
    "```\n",
    "\n",
    "Datasets can be used when you want a mix of functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataSets\n",
    "\n",
    "Just use the <code>.toDS</code> convenience method on DataFrames, RDDs or common Scala types:\n",
    "```scala\n",
    "import spark.implicits._\n",
    "\n",
    "myDF.toDS\n",
    "myRDD.toDS\n",
    "List(\"yay\", \"ohnoes\", \"hooray!\").toDS\n",
    "```\n",
    "\n",
    "On Datasets, typed operations tend to act on TypedColumns. To create a TypedColumn, all you have to do is to call <code>.as[...]</code> on your (untyped) Column:\n",
    "\n",
    "```scala\n",
    "$\"price\".as[Double] // this now represents a typed Column\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations on Datasets\n",
    "\n",
    "The Dataset API includes both untyped and typed transformations:\n",
    " - **untyped transformations** the transformations we learned on DataFrames\n",
    " - **typed transformations** typed variants of many DataFrame transformations plus additional transformatins such as RDD-like higher-order functions.\n",
    " \n",
    "Datasets are missing an important transformation that we often used on RDDs: **reduceByKey**.\n",
    "\n",
    "**Challenge:** Emulate the semantics of reduceByKey on a Dataset using Dataset operations presented so far. Assume the following data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyValues = List((3,\"Me\"), (1,\"Thi\"), (2,\"Se\"), (3,\"ssa\"), (1,\"sIsA\"), (3,\"ge:\"), (3,\"-)\"), (2,\"cre\"), (2,\"t\"))\n",
    "\n",
    "import spark.implicits._\n",
    "val keyValuesDS = keyValues.toDS\n",
    "\n",
    "keyValuesDs.groupByKey(p => p._1)\n",
    "           .mapGroups((k, vs) => (k, vs.foldLeft(\"\")((acc, p) => acc + p._2)))\n",
    "           .sort($\"_1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The only issue with this approach is this disclaimer in the API docs for mapGroups:**\n",
    "This function does not support partial aggregation, and as a result requires shuffling all the data in the Dataset. If an application intends to perform an aggregation over each key *(which is exactly what we're doing)*, it is best to use the reduce function or an org.apache.spark.sql.expressions#Aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyValues = List((3,\"Me\"), (1,\"Thi\"), (2,\"Se\"), (3,\"ssa\"), (1,\"sIsA\"), (3,\"ge:\"), (3,\"-)\"), (2,\"cre\"), (2,\"t\"))\n",
    "\n",
    "import spark.implicits._\n",
    "val keyValuesDS = keyValues.toDS\n",
    "\n",
    "keyValuesDs.groupByKey(p => p._1)\n",
    "           .mapValues(p => p._2)\n",
    "           .reduceGroups((acc, str) => acc + str)  \n",
    "           .sort($\"key\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That works, but the docs also suggested an Aggregator!**\n",
    "\n",
    "#### Aggregators\n",
    "\n",
    "\n",
    "A class that helps you generically aggregate data. Kind of like the aggregate method we saw on RDDs.\n",
    "\n",
    "```scala\n",
    "class Aggregator[-IN, BUF, OUT]\n",
    "```\n",
    "- **IN** is  the input type to the aggregator. When using an aggregator after groupByKey, this is the type that represents the value in the key/value pair.\n",
    "- **BUF** is  the intermediate type during aggregation.\n",
    "- **OUT** is the type of the ouput of the aggregation.\n",
    "\n",
    "**Emulating reduceByKey with an Aggregator:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyValues = List((3,\"Me\"), (1,\"Thi\"), (2,\"Se\"), (3,\"ssa\"), (1,\"sIsA\"), (3,\"ge:\"), (3,\"-)\"), (2,\"cre\"), (2,\"t\"))\n",
    "\n",
    "import spark.implicits._\n",
    "val keyValuesDS = keyValues.toDS\n",
    "\n",
    "import org.apache.spark.sql.expressions.Aggregator\n",
    "\n",
    "val strConcat = new Aggregator[(Int, String), String, String] {\n",
    "    def zero: String = \"\"\n",
    "    def reduce(b: String, a: (Int, String)): String = b + a._2\n",
    "    def merge(b1: String, b2: String): String = b1 + b2\n",
    "    def finish(r: String): String = r\n",
    "}.toColumn\n",
    "\n",
    "keyValuesDS.groupByKey(pair => pair._1)\n",
    "           .agg(strConcat.as[String])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are missing two method implementations. What's an Encoder?**\n",
    "\n",
    "#### Encoders\n",
    "\n",
    "Encoders are what convert your data between JVM objects and Spark SQL's specialized internal (tabular) representation. **They're required by all Datasets!**\n",
    "\n",
    "Encoders are highly specialized, optimized code generators that generate custom bytecode for serialization and deserialization of your data. The serialized data is stored using Spark internal Tungsten binary format, alllowing for operations on serialized data and improved memory utilization.\n",
    "\n",
    "**What sets them apart from regular Java or Kryo serialization:**\n",
    " - Limited to and optimal for primitives and case classes, Spark SQL data types, which are well-understood.\n",
    " - **They contain schema information**, which makes these highly optimized code generators possible, and enables optimization based on the shape pf the data. Since Spark understands the structure of data in Datasets, it can create a more optimal layout in memory when caching Datasets.\n",
    " - Uses significantly less memory than Kryo/Java serialization\n",
    " - \\>10x faster than Kryo serialization (Java serialization order of magnitude slower)\n",
    " \n",
    "**Two ways to introduce encoders:**\n",
    " - **Automatically** (generally the case) via implicits form a SparkSession.\n",
    " - **Explicitly** via <code>org.apache.spark.sql.Encoder</code>, which contains a large selection of methods for createing Encoders from Scala primitive types and Products.\n",
    " \n",
    "**Example:**\n",
    "```scala\n",
    "Encoders.scalaInt //Encoder[Int]\n",
    "Encoders.STRING // Encoder[String]\n",
    "Encoders.product[Person] // Encoder[Person], where Person extends Product/is a case class\n",
    "```\n",
    "\n",
    "**Emulating reduceByKey with an Aggregator (ctd.):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val keyValues = List((3,\"Me\"), (1,\"Thi\"), (2,\"Se\"), (3,\"ssa\"), (1,\"sIsA\"), (3,\"ge:\"), (3,\"-)\"), (2,\"cre\"), (2,\"t\"))\n",
    "\n",
    "import spark.implicits._\n",
    "val keyValuesDS = keyValues.toDS\n",
    "\n",
    "import org.apache.spark.sql.expressions.Aggregator\n",
    "import org.apache.spark.sql.{Encoder, Encoders}\n",
    "\n",
    "val strConcat = new Aggregator[(Int, String), String, String] {\n",
    "    def zero: String = \"\"\n",
    "    def reduce(b: String, a: (Int, String)): String = b + a._2\n",
    "    def merge(b1: String, b2: String): String = b1 + b2\n",
    "    def finish(r: String): String = r\n",
    "    override def bufferEncoder: Encoder[String] = Encoders.STRING\n",
    "    override def outputEncoder: Encoder[String] = Encoders.STRING\n",
    "}.toColumn\n",
    "\n",
    "keyValuesDS.groupByKey(pair => pair._1)\n",
    "           .agg(strConcat.as[String])\n",
    "           .sort($\"key\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use Datasets over DataFrames over RDDs?\n",
    "\n",
    "**Use Datasets when...**\n",
    " - you have structured/semi-structured data\n",
    " - you want typesafety\n",
    " - you need to work with functional APIs\n",
    " - you need good performance, but it doesn't have to be the best\n",
    " \n",
    "**Use DataFrames when...**\n",
    " - you have structured/semi-structured data\n",
    " - you want the best possible performance, automatically optimzied for you\n",
    " \n",
    "**Use RDDs when...**\n",
    " - you have usntructured data\n",
    " - you need to fine-tune and manage low-level details of RDD computations\n",
    " - you have complex data types that cannot be serialized with <code>Encoders</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "\n",
    "1. **Catalyst can't optimize all operations**\n",
    "\n",
    "    While **relational filter operations**, e.g. ```ds.filter($\"city\".as[String] === \"Boston\")```, can be optimized, Catalyst \n",
    "    cannot optimize **functional filter operations** like ```ds.filter(p => p.city == \"Boston\")```.\n",
    "\n",
    "\n",
    "2. **Takeaways:**\n",
    " - When using Datasets with high-order functions like map, you miss out on many Catalyst optimizations.\n",
    " - When using Datasets with relationqal operations like select, you can get all of Catalyst's optimizations.\n",
    " - Though not all operations on Datasets benefit from Catalyst's optimizations, Tungsten is still always running under the hood of Datasets, storing and organizing data in a highly optimized way, which can result in large speedups over RDDs.\n",
    "\n",
    "\n",
    "3. **Limited Data Types**\n",
    "\n",
    "4. **Requires semi-structured/structured data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
